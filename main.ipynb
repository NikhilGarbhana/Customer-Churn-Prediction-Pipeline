{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85ca668-ac7c-4376-8efd-bd5160cebdc5",
   "metadata": {},
   "source": [
    "                                        Project Title: Customer Churn Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558cd6f4-2725-4232-b95b-81cd3cfcf2a0",
   "metadata": {},
   "source": [
    "Importing the libraries that are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d769aafe-c190-4833-a13a-4fc89abe9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import random\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d609924-afd9-45b0-b896-92b179e72ec0",
   "metadata": {},
   "source": [
    "                                            Step 1 - Data Collection (Simulated Data Source)\n",
    "Weâ€™ll generate synthetic customer and transaction data using the Faker library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c46f17-91cb-4342-ba61-395d15451c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Generate synthetic customer data\n",
    "def generate_customer_data(num_records=1000):\n",
    "    data = []\n",
    "    for _ in range(num_records):\n",
    "        data.append({\n",
    "            \"customer_id\": fake.uuid4(),\n",
    "            \"name\": fake.name(),\n",
    "            \"age\": random.randint(18, 70),\n",
    "            \"signup_date\": fake.date_between(start_date='-2y', end_date='today'),\n",
    "            \"subscription_type\": random.choice([\"Basic\", \"Standard\", \"Premium\"]),\n",
    "            \"last_active_date\": fake.date_between(start_date='-1y', end_date='today'),\n",
    "            \"is_churned\": random.choice([0, 1])\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "customer_data = generate_customer_data()\n",
    "customer_data.to_csv(\"customers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9f91f-d3bc-429a-966e-5e158240f0e2",
   "metadata": {},
   "source": [
    "                                                        Step 2 - Data Ingestion \n",
    "Ingest the data into a cloud storage bucket (e.g., AWS S3) or local database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5b276a-c93b-479c-8bad-1a115c764070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "AWS_ACCESS_KEY = \"AKIA47GB76RSZX7GIJGS\"\n",
    "AWS_SECRET_KEY = \"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\"\n",
    "RAW_BUCKET = \"ng-test-csv\"\n",
    "PROCESSED_BUCKET = \"ng-test-csv-processed\"\n",
    "\n",
    "CSV_FILE_PATH = \"customers.csv\"  # Local CSV file path\n",
    "S3_OBJECT_NAME = \"customers.csv\"  # Desired S3 path\n",
    "FILE_KEY = \"transformed/customers.csv\"  # Path to the cleaned data in S3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name='eu-north-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0229de-7c96-4512-a7ba-f21d0181f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully to s3://ng-test-csv/customers.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to Upload File\n",
    "def upload_to_s3(file_path, bucket_name, object_name):\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, bucket_name, object_name)\n",
    "        print(f\"File uploaded successfully to s3://{bucket_name}/{object_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check the file path.\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"AWS credentials not available.\")\n",
    "\n",
    "# Upload the file\n",
    "upload_to_s3(CSV_FILE_PATH, RAW_BUCKET, S3_OBJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc305b7-cd7d-48cd-afe9-8ef75c6f4d8f",
   "metadata": {},
   "source": [
    "                                                            Step 3 -  ETL Pipeline\n",
    "Extracts data from the database. Transforms it into clean, analytics-ready data. Loads it into a data warehouse like Snowflake or BigQuery or AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808f7705-0ee3-436f-8652-e532122dd6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the bucket:\n",
      "customers.csv\n"
     ]
    }
   ],
   "source": [
    "# Testing S3 Connection and files\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=RAW_BUCKET)\n",
    "    print(\"Files in the bucket:\")\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        print(obj[\"Key\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6859ec5-47eb-4f77-86fe-4874aa3238a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data saved locally at /tmp/extracted_data.csv\n",
      "                            customer_id              name  age signup_date  \\\n",
      "0  a6c62624-8034-41da-9ba3-90cad8178e2c   Jessica Baldwin   31  2023-03-17   \n",
      "1  3a11ed03-03c9-4241-a7b2-4c455691669f       Paige Adams   62  2024-05-12   \n",
      "2  7cce5e08-139e-4e90-8b13-a86a35917be4   Sarah Cervantes   31  2023-08-25   \n",
      "3  bccf4e18-4b74-4b9a-a437-f5b6aaf04541  Hannah Underwood   57  2024-03-10   \n",
      "4  e69813d2-01d9-47cc-bf41-1d11dadbb1bf    Anthony Arnold   35  2024-04-10   \n",
      "\n",
      "  subscription_type last_active_date  is_churned  \n",
      "0          Standard       2024-08-13           0  \n",
      "1          Standard       2024-03-24           0  \n",
      "2          Standard       2024-08-14           1  \n",
      "3             Basic       2024-11-18           1  \n",
      "4          Standard       2024-08-01           0  \n",
      "Transformed data saved locally at /tmp/transformed_data.csv\n",
      "                            customer_id              name  age signup_date  \\\n",
      "0  a6c62624-8034-41da-9ba3-90cad8178e2c   Jessica Baldwin   31  2023-03-17   \n",
      "1  3a11ed03-03c9-4241-a7b2-4c455691669f       Paige Adams   62  2024-05-12   \n",
      "2  7cce5e08-139e-4e90-8b13-a86a35917be4   Sarah Cervantes   31  2023-08-25   \n",
      "3  bccf4e18-4b74-4b9a-a437-f5b6aaf04541  Hannah Underwood   57  2024-03-10   \n",
      "4  e69813d2-01d9-47cc-bf41-1d11dadbb1bf    Anthony Arnold   35  2024-04-10   \n",
      "\n",
      "  subscription_type last_active_date  is_churned  days_since_last_active  \n",
      "0          Standard       2024-08-13           0                     103  \n",
      "1          Standard       2024-03-24           0                     245  \n",
      "2          Standard       2024-08-14           1                     102  \n",
      "3             Basic       2024-11-18           1                       6  \n",
      "4          Standard       2024-08-01           0                     115  \n",
      "Transformed data uploaded to S3: s3://retail-sales-processed-data/transformed/customers.csv\n"
     ]
    }
   ],
   "source": [
    "def extract():\n",
    "    response = s3_client.get_object(Bucket=RAW_BUCKET, Key=\"customers.csv\")\n",
    "    raw_data = pd.read_csv(io.BytesIO(response[\"Body\"].read()))\n",
    "    raw_data.to_csv(\"tmp/extracted_data.csv\", index=False)\n",
    "    print(\"Extracted data saved locally at /tmp/extracted_data.csv\")\n",
    "    return raw_data\n",
    "\n",
    "# Extract data from S3\n",
    "extracted_data = extract()\n",
    "print(extracted_data.head())\n",
    "\n",
    "def transform():\n",
    "    df = pd.read_csv(\"tmp/extracted_data.csv\")\n",
    "    df[\"days_since_last_active\"] = (datetime.datetime.now() - pd.to_datetime(df[\"last_active_date\"])).dt.days\n",
    "    df.to_csv(\"tmp/transformed_data.csv\", index=False)\n",
    "    print(\"Transformed data saved locally at /tmp/transformed_data.csv\")\n",
    "    return df\n",
    "\n",
    "# Tranform the data as per requirement\n",
    "transformed_data = transform()\n",
    "print(transformed_data.head())\n",
    "\n",
    "def load():\n",
    "    transformed_data = pd.read_csv(\"tmp/transformed_data.csv\")\n",
    "    csv_buffer = io.StringIO()\n",
    "    transformed_data.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=PROCESSED_BUCKET, Key=\"transformed/customers.csv\", Body=csv_buffer.getvalue())\n",
    "    print(\"Transformed data uploaded to S3: s3://retail-sales-processed-data/transformed/customers.csv\")\n",
    "\n",
    "# Load the transformed data to S3 bucket\n",
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1800d-1519-43d4-bf97-2f51c29ff729",
   "metadata": {},
   "source": [
    "                                                    Step 4 - Feature Engineering and Modeling\n",
    "Prepare features and train a machine learning model for churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29241524-8763-479a-b8bf-086ee5514a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.495\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from S3\n",
    "response = s3_client.get_object(Bucket=PROCESSED_BUCKET, Key=FILE_KEY)\n",
    "df = pd.read_csv(io.BytesIO(response[\"Body\"].read()))\n",
    "\n",
    "# Feature selection\n",
    "X = df[[\"age\", \"days_since_last_active\"]]\n",
    "y = df[\"is_churned\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8539c8b1-a7de-40ac-8730-c5f2d2675af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['churn_model.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the trained model\n",
    "joblib.dump(clf, \"churn_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3de01-5e70-423b-a103-9af6b96f0edc",
   "metadata": {},
   "source": [
    "                                                            Step 5 - Model Deployment\n",
    "Deploy the churn prediction model using FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72278bd3-1b58-48db-83a8-e00e90e65536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [50380]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59145 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59394 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [50380]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Apply nest_asyncio to enable FastAPI to run inside Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define input model for the POST request\n",
    "class ChurnInput(BaseModel):\n",
    "    age: int\n",
    "    days_since_last_active: int\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the churn prediction model (ensure this file is present)\n",
    "model = joblib.load(\"churn_model.pkl\")\n",
    "\n",
    "# Define the home route\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\"message\": \"Churn Prediction API is running!\"}\n",
    "\n",
    "# Define the prediction route (POST method)\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: ChurnInput):\n",
    "    df = pd.DataFrame([data.model_dump()])\n",
    "    prediction = model.predict(df)\n",
    "    return {\"churn_prediction\": int(prediction[0])}\n",
    "\n",
    "# Start the FastAPI server using uvicorn (without 'reload')\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5564a5-b82d-4f9a-b00c-1dab193d2323",
   "metadata": {},
   "source": [
    "                                                                Step 6 - Visualization\n",
    "Build a real-time dashboard using Plotly Dash to display churn insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3eacc01-6ffc-4cae-9b24-b85ccbc7e054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2403dc09220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "import pandas as pd\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Load data\n",
    "# df = pd.read_sql(\"SELECT * FROM cleaned_customers\", con=engine)\n",
    "response = s3_client.get_object(Bucket=PROCESSED_BUCKET, Key=FILE_KEY)\n",
    "df = pd.read_csv(io.BytesIO(response[\"Body\"].read()))\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Customer Churn Insights\"),\n",
    "    dcc.Graph(\n",
    "        figure={\n",
    "            \"data\": [\n",
    "                {\"x\": df[\"subscription_type\"], \"y\": df[\"is_churned\"], \"type\": \"bar\", \"name\": \"Churned\"}\n",
    "            ],\n",
    "            \"layout\": {\"title\": \"Churn by Subscription Type\"}\n",
    "        }\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
